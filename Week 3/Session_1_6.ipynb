{"cells":[{"cell_type":"markdown","metadata":{"id":"jGHnHFav4lGi"},"source":["# Projection, Layernorm, Dropout\n","\n","<table align=\"left\">\n","  <td>\n","    <a href=\"https://colab.research.google.com/github/SkillSurf/introduction_genAI/blob/main/notebooks/S1/Session_1_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","  </td>\n","</table>\n","\n","<br />\n","<br />"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"TzvEm56B4cd1","executionInfo":{"status":"ok","timestamp":1721056366632,"user_tz":-330,"elapsed":7487,"user":{"displayName":"Mihiruth Sehara","userId":"09933473335315989515"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"markdown","metadata":{"id":"rhgC0PRC5a8Q"},"source":["### Hyperparameters"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1721056366632,"user":{"displayName":"Mihiruth Sehara","userId":"09933473335315989515"},"user_tz":-330},"id":"U-hPaOJH5aFU","outputId":"5d15322a-0898-4450-8d8d-d22e50c3063d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7b734827c470>"]},"metadata":{},"execution_count":2}],"source":["'''Hyperparameters for smaller model'''\n","\n","B = 32 # B: how many independent sequences will we process in parallel?\n","T = 8  # T: what is the maximum context length for predictions?\n","C = 32 # C: numer of different features analysed (also D = dims)\n","H = 4  # H: number of attention heads\n","L = 4  # L: Number of layers\n","learning_rate = 1e-3\n","\n","'''Final Hyperparameters'''\n","\n","# B = 64 # B: how many independent sequences will we process in parallel?\n","# T = 256  # T: what is the maximum context length for predictions?\n","# H = 6\n","# C = 64*H\n","# L = 6\n","# learning_rate = 1e-4\n","\n","# Common Hyperparameters\n","max_iters = 5000\n","eval_interval = 500\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","dropout = 0.2\n","torch.manual_seed(1337)"]},{"cell_type":"markdown","metadata":{"id":"yLFQvyFf4taN"},"source":["### Data"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":728,"status":"ok","timestamp":1721056367357,"user":{"displayName":"Mihiruth Sehara","userId":"09933473335315989515"},"user_tz":-330},"id":"NwgGJ0-K4rx2","outputId":"20b405fa-5370-4250-d339-eac69574b23e"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-07-15 15:12:46--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","input.txt           100%[===================>]   1.06M  5.52MB/s    in 0.2s    \n","\n","2024-07-15 15:12:47 (5.52 MB/s) - ‘input.txt’ saved [1115394/1115394]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2063,"status":"ok","timestamp":1721056369417,"user":{"displayName":"Mihiruth Sehara","userId":"09933473335315989515"},"user_tz":-330},"id":"Hbw16RqT4wFg","outputId":"5654ae4c-4145-4162-deeb-93dd39a4da40"},"outputs":[{"output_type":"stream","name":"stdout","text":["vocab_size: 65\n","vocabulary: \n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"]}],"source":["with open('input.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","chars_str = ''.join(chars)\n","print(f'vocab_size: {vocab_size}')\n","print(f'vocabulary: {chars_str}')\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - T, (B,))\n","    x = torch.stack([data[i:i+T] for i in ix])\n","    y = torch.stack([data[i+1:i+T+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n"]},{"cell_type":"markdown","metadata":{"id":"djRweqFN43In"},"source":["### Head, MHSA"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"84SgLLFc4xt6","executionInfo":{"status":"ok","timestamp":1721056369418,"user_tz":-330,"elapsed":3,"user":{"displayName":"Mihiruth Sehara","userId":"09933473335315989515"}}},"outputs":[],"source":["class Head(nn.Module):\n","    \"\"\" One head of self attention\"\"\"\n","\n","    def __init__(self, Ci, Co):\n","        super().__init__()\n","        self.key   = nn.Linear(Ci, Co, bias=False)\n","        self.query = nn.Linear(Ci, Co, bias=False)\n","        self.value = nn.Linear(Ci, Co, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n","\n","    def forward(self, x):\n","        B, T, Ci  = x.shape\n","        '''\n","        B  - batch               # of independant vectors processed\n","        T  - time/block/context  # of tokens in a context\n","        Ci - channals/dims input # of features in input\n","        '''\n","\n","        k = self.key(x)   # (B,T,Co)\n","        q = self.query(x) # (B,T,Co)\n","\n","        # compute attention scores / affinities\n","        wei = q @ k.transpose(-2,-1)                                 # (B,T,Co) @ (B,Co,T) -> (B,T,T)\n","        wei /= C**0.5                                                # (B,T,T) scaling, bring variance to 1, to prevent softmax clipping\n","        wei  = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))   # (B,T,T) Replace upper triangular of wei with -inf\n","        wei  = F.softmax(wei, dim=-1)                                # (B,T,T) -inf -> 0, rest normalized to 1\n","\n","        v = self.value(x)  # (B,T,Co)\n","        out = wei @ v      # (B,T,T) @ (B,T,Co) = (B,T,Co)\n","\n","        return out\n","\n","\n","class MultiHeadAttention(nn.Module):\n","\n","    def __init__(self, Ci, H, head_size):\n","        super().__init__()\n","        # 4 heads of 8-dimensional self-attention, for n_embed=32, like a group convolution\n","        self.heads = nn.ModuleList([Head(Ci=Ci, Co=head_size) for _ in range(H)])\n","\n","    def forward(self, x):\n","        x = torch.cat([h(x) for h in self.heads], dim=-1)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"Om6pUr4N463t"},"source":["### Transformer Block"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2m3aC0tt44sv","executionInfo":{"status":"ok","timestamp":1721056369418,"user_tz":-330,"elapsed":3,"user":{"displayName":"Mihiruth Sehara","userId":"09933473335315989515"}}},"outputs":[],"source":["class Block(nn.Module):\n","    ''' Transformer block: communication followed by computation '''\n","\n","    def __init__(self, C, H): # C: embedding dimension, H: number of heads\n","        super().__init__()\n","        self.ln1 = nn.LayerNorm(C)   # Layernorm along channels (batch & time are batch dims): y = beta + gamma * [x-E(x)]/sqrt(V(x) + ep)\n","        self.sa = MultiHeadAttention(Ci=C, H=H, head_size=C//H)\n","        self.ln2 = nn.LayerNorm(C)\n","        self.ffwd = nn.Sequential(         # Feedforward network, so the tokens can \"think about\" what they found in attention.\n","            nn.Linear(C, C*4),\n","            nn.ReLU(),\n","            nn.Linear(C*4, C),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        # Residual connections around MSA & FF, to help training\n","        # Note: input without layernorm is added to output\n","\n","        x_skip = x\n","\n","        x = self.ln1(x)\n","        x = self.sa(x)   # (B,T,C), Multi head self attention\n","        x = x + x_skip\n","\n","        x = self.ln2(x)\n","        x = self.ffwd(x) # (B,T,C), Per token level. B,T act as batch dimensions\n","        x = x + x_skip\n","\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"rEZ6CZrm5ADW"},"source":["### Model"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"8B84EGMO49AA","executionInfo":{"status":"ok","timestamp":1721056369418,"user_tz":-330,"elapsed":3,"user":{"displayName":"Mihiruth Sehara","userId":"09933473335315989515"}}},"outputs":[],"source":["class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self, B,T,C,H,L):\n","        super().__init__()\n","        self.B, self.T, self.C, self.H, self.L = B,T,C,H,L\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, C) # for every possible token, weights for next token\n","        self.position_embedding_table = nn.Embedding(T, C)\n","\n","        self.blocks  = nn.Sequential(*[Block(C, H) for _ in range(L)])\n","        self.ln_final = nn.LayerNorm(C)\n","        self.lm_head = nn.Linear(C, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","\n","        tok_emb = self.token_embedding_table(idx)                                    # (B,T,C=n_embed)\n","        pos_emb = self.position_embedding_table(torch.arange(self.T, device=device)) # (T,C): [0,1,2..T-1]\n","\n","        x = tok_emb + pos_emb     # (B,T,C)\n","        x = self.blocks(x)\n","        x = self.ln_final(x)      # Layernorm applied before last\n","        logits = self.lm_head(x)  # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        for _ in range(max_new_tokens):                        # idx is (B, T) array of indices in the current context\n","            idx_cond = idx[:, -self.T:]                        # crop the last block_size tokens for input\n","            logits, loss = self(idx_cond)                      # get the predictions\n","            logits = logits[:, -1, :]                          # (B,T,C) -> (B, C)\n","            probs = F.softmax(logits, dim=-1)                  # (B, C)\n","            idx_next = torch.multinomial(probs, num_samples=1) # sample from the distribution acc to prob (B, 1)\n","            idx = torch.cat((idx, idx_next), dim=1)            # New idx is concat (B, T+1)\n","        return idx\n","\n","model = BigramLanguageModel(B,T,C,H,L)\n","m = model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"GqlT4Bwa5Dfo"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9pDDJiTV5BPa","outputId":"c34bcceb-876f-433d-a552-7b26aa96ba82"},"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 4.3777, val loss 4.3759\n","step 500: train loss 2.4585, val loss 2.4434\n","step 1000: train loss 2.2973, val loss 2.3072\n","step 1500: train loss 2.2060, val loss 2.2323\n","step 2000: train loss 2.1624, val loss 2.1886\n","step 2500: train loss 2.1125, val loss 2.1715\n","step 3000: train loss 2.1060, val loss 2.1516\n","step 3500: train loss 2.0893, val loss 2.1324\n","step 4000: train loss 2.0550, val loss 2.1122\n","step 4500: train loss 2.0460, val loss 2.1017\n"]}],"source":["@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","    if iter % eval_interval == 0:   # every once in a while evaluate the loss on train and val sets\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    xb, yb = get_batch('train')     # sample a batch of data\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"tLhWVvJR5Hmv"},"source":["### Inference"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16651,"status":"ok","timestamp":1721056613100,"user":{"displayName":"Mihiruth Sehara","userId":"09933473335315989515"},"user_tz":-330},"id":"oArZB_RT5FHn","outputId":"a890d342-f51c-4ae0-c11e-9d0167f4c9c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["        the myy suppen,\n","He spatuld I and air wive,\n","Rnot son it for you kndoth saay,\n","Of thany of WAnd de as you t;\n","As a-toood woord:\n","Yof spobesws quh eerendaut all, me ord?\n","\n","ABENRBRok in in the mde whoure thore blord age, have trimem'ty bond ver:\n","Bets for cevenm-are suck my sing\n","But taick yous yeathing.-\n","I crame\n","And stled feay\n","that thing boot cloove,\n","Of was faread:\n","Thou cand or to Her.\n","\n","\n","CALORK:\n","It, neTtly, fead not\n","I pleares;\n","I wat'd-- adlaysmy is'd sen,\n","Lich, fromine:\n","And purt blaw thou all there be him ca hid;\n","Singe flance,: to be grace!\n","Aw tycend dyest me how me, So, ove come Wsay;\n","OrLok be wan he pyertied word thou on's frouseny,\n","Eid thams is panows'rer, adauthat hisap,\n","\n","Somenuars;\n","And:\n","Your theeds a, well you my dy with\n","The lorturraseg so be age take gaie\n","ath not my cived bele my merks 'lut lettenncaw's hy, at asittuch arang usting of the kervill yee's tur arace.\n","\n","LOUMO:\n","\n","Hoppady crand to Roubereat: bitth the thou? of your jet, dow. Whule, an t, verjuses a more,\n","GoRmaek's my\n","Tresesit drencouf with my Bodrotitle\n","tair, boidsink.\n","\n","KIVCNBBR COUd sbe.\n","\n","KINVZGLIZIUrim'd the o'--to breane rrver reir, evear of ot Louctals, hand selse\n","To lort, huf redise wablatady! Mare tomen ever'd same, and be so, ay to not have your to didosear.\n","\n","CORCOTUCI:-----recelsy carured will is way,\n","Furent, to whe thou singe;\n","So sand:\n","Back a ise:\n","For cor,\n","Sto pplabed.\n","Dirry cos it, andtull I so ely cay.\n","For I wer.\n","\n","MEROWIANUS:\n","Mery sor bod sall? I way,\n","To knot you the with whe me my land bloind, aor geto can the trower.\n","\n","Plizencendes ar sel cupul it in not afallsern, ble the pgayeate,\n","And it The ord.\n","\n","COMER: figed andias,\n","Four heF with in grourser?\n","\n","GLAUrgoo-the parle lord not, con ming igh-untrives the aglais harote, wilgse,\n",": my leqeadr: rown trby your at of and so tham b, ge; are of gram;\n","Ifle is he my sweavl,\n","Thorrry:------\n","Ygethee, as isbe stry ben with aroung\n","Sucupverat onder deseave, thou, zand stime his Grecifs.\n","\n","HAUTh to-grat,\n","Trar lep.\n","\n","DARDOUDWews.\n","Or begy shonges.\n","\n","IVMICINTIE:\n","Wadbchroms o\n"]}],"source":["context = torch.ones((1, T), dtype=torch.long, device=device)  # start with '\\n\\n\\n\\n' as seed\n","out_ints = m.generate(context, max_new_tokens=2000)[0].tolist() # output list of ints\n","print(decode(out_ints))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fNpRB1u5IuS"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/SkillSurf/introduction_genAI/blob/main/notebooks/S1/Session_1_6.ipynb","timestamp":1721053488006}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}